#!/usr/bin/env python3
"""
GPU-OPTIMIZED SCANNER IDENTIFICATION - TARGET 95%+ ACCURACY
Ultra-fast GPU-accelerated version using TensorFlow 2.16, CuPy, and GPU-optimized ML
"""

import os
import sys
import time
import signal
import logging
import warnings
from datetime import datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from contextlib import contextmanager
import threading

# GPU and optimization imports
import tensorflow as tf
import cupy as cp
from tensorflow import keras
from tensorflow.keras import layers, models, optimizers, callbacks

# ML libraries
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.impute import KNNImputer
import xgboost as xgb
import lightgbm as lgb
import pickle

warnings.filterwarnings('ignore')

# Global flag for interruption
interrupted = False

def signal_handler(signum, frame):
    """Handle interruption signals"""
    global interrupted
    interrupted = True
    logger.info("ðŸ›‘ Interrupt received! Stopping gracefully...")
    
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

@contextmanager
def timeout_context(seconds):
    """Context manager for timeouts"""
    def timeout_handler():
        raise TimeoutError(f"Operation timed out after {seconds} seconds")
    
    timer = threading.Timer(seconds, timeout_handler)
    timer.start()
    try:
        yield
    finally:
        timer.cancel()

# Configure GPU
def configure_gpu():
    """Configure GPU settings for optimal performance"""
    # Check GPU availability
    physical_devices = tf.config.list_physical_devices('GPU')
    if not physical_devices:
        logger.warning("No GPU devices found! Running on CPU...")
        return False
    
    logger.info(f"Found {len(physical_devices)} GPU device(s):")
    for i, device in enumerate(physical_devices):
        logger.info(f"  GPU {i}: {device}")
    
    # Configure GPU memory growth
    try:
        for device in physical_devices:
            tf.config.experimental.set_memory_growth(device, True)
        
        # Use GPU 2 (which is free according to nvidia-smi)
        tf.config.set_visible_devices(physical_devices[2], 'GPU')
        logger.info("âœ… Configured to use GPU 2 (Tesla K80)")
        
        # Enable mixed precision for faster training
        tf.keras.mixed_precision.set_global_policy('mixed_float16')
        logger.info("âœ… Enabled mixed precision training")
        
        return True
    except Exception as e:
        logger.error(f"GPU configuration failed: {e}")
        return False

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Set random seeds
np.random.seed(42)
tf.random.set_seed(42)

class GPUOptimized95Plus:
    def __init__(self):
        self.random_seed = 42
        self.output_path = "gpu_optimized_results"
        self.start_time = datetime.now()
        
        # Configure GPU
        self.gpu_available = configure_gpu()
        
        # Create output directories
        os.makedirs(f"{self.output_path}/models", exist_ok=True)
        os.makedirs(f"{self.output_path}/results", exist_ok=True)
        os.makedirs(f"{self.output_path}/visualizations", exist_ok=True)
        
        # Initialize components
        self.label_encoder = LabelEncoder()
        self.scaler = StandardScaler()
        
    def load_features_gpu_optimized(self):
        """Load and prepare features with GPU optimization"""
        logger.info("="*80)
        logger.info("ðŸš€ LOADING FEATURES FOR GPU-OPTIMIZED 95%+ ACCURACY")
        logger.info("="*80)
        
        # Load features
        features_file = "features/feature_vectors/scanner_features.csv"
        if not os.path.exists(features_file):
            logger.error(f"Features file not found: {features_file}")
            return None
            
        df = pd.read_csv(features_file)
        logger.info(f"Loaded {len(df)} samples with {len(df.columns)-5} features")
        
        # Create stratified split
        df['split'] = 'unknown'
        train_idx, test_idx = train_test_split(
            range(len(df)), 
            test_size=0.15, 
            random_state=self.random_seed,
            stratify=df['scanner_id']
        )
        
        df.iloc[train_idx, df.columns.get_loc('split')] = 'train'
        df.iloc[test_idx, df.columns.get_loc('split')] = 'test'
        
        split_counts = df['split'].value_counts()
        logger.info(f"Stratified split: {split_counts}")
        
        return df
    
    def gpu_feature_engineering(self, df):
        """GPU-accelerated feature engineering using CuPy"""
        logger.info("ðŸ”¥ GPU-ACCELERATED FEATURE ENGINEERING")
        
        # Separate features from metadata
        feature_columns = [col for col in df.columns 
                          if col not in ['filepath', 'scanner_id', 'dpi', 'source', 'split']]
        
        X = df[feature_columns].values
        y = df['scanner_id'].values
        split = df['split'].values
        
        # Handle missing values
        logger.info("Imputing missing values...")
        imputer = KNNImputer(n_neighbors=5)
        X = imputer.fit_transform(X)
        
        # Convert to GPU arrays if possible
        try:
            import cupy as cp
            if self.gpu_available:
                logger.info("Transferring data to GPU...")
                X_gpu = cp.asarray(X)
                
                # GPU-accelerated feature selection using correlation
                logger.info("GPU-accelerated feature correlation analysis...")
                corr_matrix = cp.corrcoef(X_gpu.T)
                
                # Remove highly correlated features (>0.95)
                high_corr_pairs = cp.where(cp.abs(corr_matrix) > 0.95)
                features_to_remove = set()
                for i, j in zip(high_corr_pairs[0], high_corr_pairs[1]):
                    if i != j and i not in features_to_remove:
                        features_to_remove.add(j)
                
                # Keep features
                features_to_keep = [i for i in range(X_gpu.shape[1]) if i not in features_to_remove]
                X_gpu_filtered = X_gpu[:, features_to_keep]
                
                logger.info(f"Removed {len(features_to_remove)} highly correlated features")
                logger.info(f"Remaining features: {X_gpu_filtered.shape[1]}")
                
                # Transfer back to CPU for sklearn compatibility
                X = cp.asnumpy(X_gpu_filtered)
                
        except Exception as e:
            logger.warning(f"GPU processing failed, using CPU: {e}")
        
        # Split data
        train_mask = split == 'train'
        test_mask = split == 'test'
        
        X_train = X[train_mask]
        X_test = X[test_mask]
        y_train = y[train_mask]
        y_test = y[test_mask]
        
        # Encode labels
        y_train_encoded = self.label_encoder.fit_transform(y_train)
        y_test_encoded = self.label_encoder.transform(y_test)
        
        # Advanced feature selection
        logger.info("Selecting top features...")
        selector = SelectKBest(mutual_info_classif, k=min(100, X_train.shape[1]))
        X_train_selected = selector.fit_transform(X_train, y_train_encoded)
        X_test_selected = selector.transform(X_test)
        
        # Scale features
        logger.info("Scaling features...")
        X_train_scaled = self.scaler.fit_transform(X_train_selected)
        X_test_scaled = self.scaler.transform(X_test_selected)
        
        logger.info(f"Final feature dimensions: {X_train_scaled.shape[1]}")
        logger.info(f"Training set: {X_train_scaled.shape}")
        logger.info(f"Test set: {X_test_scaled.shape}")
        logger.info(f"Number of classes: {len(self.label_encoder.classes_)}")
        
        return X_train_scaled, X_test_scaled, y_train_encoded, y_test_encoded
    
    def create_gpu_optimized_models(self):
        """Create GPU-optimized ML models"""
        logger.info("Creating GPU-optimized models...")
        
        models = {}
        
        # 1. XGBoost with proper GPU configuration
        # Try multiple GPU configurations for XGBoost
        xgb_created = False
        
        # Method 1: Try newer XGBoost GPU API
        if not xgb_created:
            try:
                models['XGBoost_GPU'] = xgb.XGBClassifier(
                    n_estimators=1500,  # Reduced for faster testing
                    max_depth=10,
                    learning_rate=0.08,
                    subsample=0.9,
                    colsample_bytree=0.8,
                    reg_alpha=0.1,
                    reg_lambda=0.1,
                    tree_method='gpu_hist',
                    device='cuda:2',  # Use GPU 2
                    random_state=self.random_seed,
                    eval_metric='mlogloss'
                )
                logger.info("âœ… XGBoost GPU (cuda:2) configuration successful")
                xgb_created = True
            except Exception as e:
                logger.warning(f"XGBoost cuda:2 failed: {e}")
        
        # Method 2: Try legacy GPU configuration
        if not xgb_created:
            try:
                models['XGBoost_GPU'] = xgb.XGBClassifier(
                    n_estimators=1500,
                    max_depth=10,
                    learning_rate=0.08,
                    subsample=0.9,
                    colsample_bytree=0.8,
                    reg_alpha=0.1,
                    reg_lambda=0.1,
                    tree_method='gpu_hist',
                    gpu_id=2,  # Legacy API
                    random_state=self.random_seed,
                    eval_metric='mlogloss'
                )
                logger.info("âœ… XGBoost GPU (legacy gpu_id=2) configuration successful")
                xgb_created = True
            except Exception as e:
                logger.warning(f"XGBoost legacy GPU failed: {e}")
        
        # Method 3: Fallback to optimized CPU
        if not xgb_created:
            models['XGBoost_CPU_Fast'] = xgb.XGBClassifier(
                n_estimators=1500,
                max_depth=10,
                learning_rate=0.08,
                subsample=0.9,
                colsample_bytree=0.8,
                reg_alpha=0.1,
                reg_lambda=0.1,
                tree_method='hist',  # Fast CPU method
                random_state=self.random_seed,
                n_jobs=-1,
                eval_metric='mlogloss'
            )
            logger.info("âš ï¸ Using XGBoost optimized CPU version")
        
        # 2. LightGBM with robust GPU configuration
        lgb_created = False
        
        # Method 1: Try GPU with conservative settings
        if not lgb_created:
            try:
                models['LightGBM_GPU'] = lgb.LGBMClassifier(
                    n_estimators=1200,  # Reduced to prevent hanging
                    max_depth=8,        # Reduced for stability
                    learning_rate=0.08,
                    subsample=0.85,
                    colsample_bytree=0.8,
                    reg_alpha=0.1,
                    reg_lambda=0.1,
                    device='gpu',
                    gpu_device_id=2,
                    gpu_use_dp=False,    # Use single precision for speed
                    random_state=self.random_seed,
                    n_jobs=4,            # Reduced for stability
                    verbose=-1,
                    force_row_wise=True  # Prevent hanging
                )
                logger.info("âœ… LightGBM GPU (conservative) configuration successful")
                lgb_created = True
            except Exception as e:
                logger.warning(f"LightGBM GPU conservative failed: {e}")
        
        # Method 2: Try different GPU
        if not lgb_created:
            try:
                models['LightGBM_GPU'] = lgb.LGBMClassifier(
                    n_estimators=1200,
                    max_depth=8,
                    learning_rate=0.08,
                    subsample=0.85,
                    colsample_bytree=0.8,
                    device='gpu',
                    gpu_device_id=3,    # Try GPU 3
                    gpu_use_dp=False,
                    random_state=self.random_seed,
                    n_jobs=4,
                    verbose=-1,
                    force_row_wise=True
                )
                logger.info("âœ… LightGBM GPU (gpu_id=3) configuration successful")
                lgb_created = True
            except Exception as e:
                logger.warning(f"LightGBM GPU 3 failed: {e}")
        
        # Method 3: CPU fallback with optimization
        if not lgb_created:
            models['LightGBM_CPU_Fast'] = lgb.LGBMClassifier(
                n_estimators=1500,
                max_depth=10,
                learning_rate=0.06,
                subsample=0.9,
                colsample_bytree=0.8,
                reg_alpha=0.1,
                reg_lambda=0.1,
                device='cpu',
                random_state=self.random_seed,
                n_jobs=-1,
                verbose=-1
            )
            logger.info("âš ï¸ Using LightGBM optimized CPU version")
        
        # 3. Enhanced Gradient Boosting (CPU - still very fast)
        models['GradientBoosting_Ultra'] = GradientBoostingClassifier(
            n_estimators=1500,
            max_depth=15,
            learning_rate=0.03,
            subsample=0.9,
            max_features='sqrt',
            random_state=self.random_seed
        )
        
        # 4. Ultra Random Forest
        models['RandomForest_Ultra'] = RandomForestClassifier(
            n_estimators=2000,
            max_depth=None,
            min_samples_split=2,
            min_samples_leaf=1,
            max_features='sqrt',
            class_weight='balanced',
            oob_score=True,
            random_state=self.random_seed,
            n_jobs=-1
        )
        
        return models
    
    def create_gpu_cnn(self, num_classes):
        """Create GPU-optimized CNN for image patches"""
        if not self.gpu_available:
            logger.warning("No GPU available for CNN training")
            return None
        
        logger.info("Creating GPU-optimized CNN...")
        
        # Efficient CNN architecture (try GPU, fallback to CPU)
        try:
            device = '/GPU:2' if self.gpu_available else '/CPU:0'
        except:
            device = '/CPU:0'
        
        with tf.device(device):
            model = models.Sequential([
                # Input layer
                layers.Input(shape=(64, 64, 3)),
                
                # Efficient convolution blocks
                layers.Conv2D(64, (7, 7), strides=2, padding='same', activation='relu'),
                layers.BatchNormalization(),
                layers.MaxPooling2D((3, 3), strides=2, padding='same'),
                
                # Depthwise separable convolutions for efficiency
                layers.SeparableConv2D(128, (3, 3), padding='same', activation='relu'),
                layers.BatchNormalization(),
                layers.MaxPooling2D((2, 2)),
                
                layers.SeparableConv2D(256, (3, 3), padding='same', activation='relu'),
                layers.BatchNormalization(),
                layers.MaxPooling2D((2, 2)),
                
                layers.SeparableConv2D(512, (3, 3), padding='same', activation='relu'),
                layers.BatchNormalization(),
                layers.GlobalAveragePooling2D(),
                
                # Classification head
                layers.Dense(512, activation='relu'),
                layers.BatchNormalization(),
                layers.Dropout(0.5),
                
                layers.Dense(256, activation='relu'),
                layers.BatchNormalization(),
                layers.Dropout(0.3),
                
                layers.Dense(num_classes, activation='softmax', dtype='float32')  # Ensure float32 output
            ])
        
        # Compile with mixed precision optimizer
        optimizer = optimizers.Adam(learning_rate=0.001)
        
        model.compile(
            optimizer=optimizer,
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def load_image_patches_gpu(self, df):
        """Load image patches for CNN training"""
        logger.info("Loading image patches for GPU CNN...")
        
        train_df = df[df['split'] == 'train'].sample(n=min(1000, len(df[df['split'] == 'train'])))  # Sample for speed
        test_df = df[df['split'] == 'test'].sample(n=min(200, len(df[df['split'] == 'test'])))
        
        def load_patches(df_subset):
            patches = []
            labels = []
            
            for _, row in df_subset.iterrows():
                try:
                    if os.path.exists(row['filepath']):
                        import cv2
                        img = cv2.imread(row['filepath'])
                        if img is not None:
                            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                            img = cv2.resize(img, (64, 64))
                            patches.append(img)
                            labels.append(row['scanner_id'])
                except:
                    continue
            
            return np.array(patches), np.array(labels)
        
        X_train_img, y_train_img = load_patches(train_df)
        X_test_img, y_test_img = load_patches(test_df)
        
        # Normalize
        X_train_img = X_train_img.astype(np.float32) / 255.0
        X_test_img = X_test_img.astype(np.float32) / 255.0
        
        # Encode labels
        y_train_img_encoded = self.label_encoder.transform(y_train_img)
        y_test_img_encoded = self.label_encoder.transform(y_test_img)
        
        logger.info(f"Loaded {len(X_train_img)} training patches, {len(X_test_img)} test patches")
        
        return X_train_img, X_test_img, y_train_img_encoded, y_test_img_encoded
    
    def train_gpu_cnn(self, model, X_train, X_test, y_train, y_test):
        """Train CNN on GPU with optimizations"""
        logger.info("ðŸ”¥ Training CNN on GPU with mixed precision...")
        
        # GPU-optimized callbacks
        callbacks_list = [
            callbacks.ModelCheckpoint(
                f"{self.output_path}/models/gpu_cnn_best.keras",
                monitor='val_accuracy',
                save_best_only=True,
                mode='max'
            ),
            callbacks.EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True
            ),
            callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=1e-7
            )
        ]
        
        # Train with GPU optimization
        history = model.fit(
            X_train, y_train,
            batch_size=64,  # Larger batch size for GPU efficiency
            epochs=50,
            validation_data=(X_test, y_test),
            callbacks=callbacks_list,
            verbose=1
        )
        
        return history
    
    def rapid_evaluation(self, models, X_train, X_test, y_train, y_test):
        """Rapid evaluation focusing on best models"""
        logger.info("="*80)
        logger.info("ðŸš€ RAPID GPU-OPTIMIZED EVALUATION FOR 95%+")
        logger.info("="*80)
        
        results = {}
        
        for name, model in models.items():
            start_time = time.time()
            logger.info(f"âš¡ Evaluating {name}...")
            
            try:
                # Train model
                model.fit(X_train, y_train)
                
                # Quick prediction
                y_pred = model.predict(X_test)
                accuracy = accuracy_score(y_test, y_pred)
                
                # Fast 3-fold CV for validation
                cv_scores = cross_val_score(
                    model, X_train, y_train,
                    cv=3, scoring='accuracy', n_jobs=-1
                )
                
                elapsed = time.time() - start_time
                
                results[name] = {
                    'test_accuracy': accuracy,
                    'cv_mean': np.mean(cv_scores),
                    'cv_std': np.std(cv_scores),
                    'training_time': elapsed
                }
                
                logger.info(f"âœ… {name}: {accuracy:.4f} ({accuracy*100:.2f}%) in {elapsed:.1f}s")
                
                if accuracy >= 0.95:
                    logger.info(f"ðŸŽ‰ {name} ACHIEVED 95%+ ACCURACY! ðŸŽ‰")
                
            except Exception as e:
                logger.error(f"âŒ {name} failed: {e}")
                results[name] = {'test_accuracy': 0.0, 'error': str(e)}
        
        return results
    
    def create_visualizations_gpu(self, results):
        """Create performance visualizations"""
        logger.info("Creating performance visualizations...")
        
        # Extract successful results
        successful_results = {k: v for k, v in results.items() if 'error' not in v}
        
        if not successful_results:
            logger.error("No successful results to visualize")
            return
        
        models = list(successful_results.keys())
        accuracies = [successful_results[m]['test_accuracy'] for m in models]
        
        # Create performance chart
        plt.figure(figsize=(12, 8))
        
        bars = plt.bar(models, accuracies, color=['gold' if acc >= 0.95 else 'skyblue' for acc in accuracies])
        plt.axhline(y=0.95, color='red', linestyle='--', linewidth=2, label='95% Target')
        plt.title('GPU-Optimized Scanner Identification Results', fontsize=16, fontweight='bold')
        plt.ylabel('Accuracy', fontsize=12)
        plt.xlabel('Models', fontsize=12)
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        plt.legend()
        
        # Add accuracy labels
        for bar, acc in zip(bars, accuracies):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, 
                    f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(f"{self.output_path}/visualizations/gpu_results.png", dpi=300, bbox_inches='tight')
        plt.close()
        
    def run_gpu_optimized_pipeline(self):
        """Run the complete GPU-optimized pipeline"""
        logger.info("="*80)
        logger.info("ðŸš€ GPU-OPTIMIZED SCANNER IDENTIFICATION PIPELINE")
        logger.info(f"GPU Available: {self.gpu_available}")
        logger.info("TARGET: 95%+ ACCURACY with MAXIMUM SPEED")
        logger.info("="*80)
        
        start_time = datetime.now()
        
        # 1. Load and prepare features
        df = self.load_features_gpu_optimized()
        if df is None:
            return
        
        # 2. GPU-accelerated feature engineering
        X_train, X_test, y_train, y_test = self.gpu_feature_engineering(df)
        
        # 3. Create GPU-optimized models
        models = self.create_gpu_optimized_models()
        
        # 4. Rapid evaluation
        results = self.rapid_evaluation(models, X_train, X_test, y_train, y_test)
        
        # 5. Train CNN if GPU available
        cnn_results = {}
        if self.gpu_available:
            try:
                logger.info("ðŸ”¥ Training GPU-optimized CNN...")
                num_classes = len(self.label_encoder.classes_)
                cnn_model = self.create_gpu_cnn(num_classes)
                
                X_train_img, X_test_img, y_train_img, y_test_img = self.load_image_patches_gpu(df)
                
                if len(X_train_img) > 0:
                    history = self.train_gpu_cnn(cnn_model, X_train_img, X_test_img, y_train_img, y_test_img)
                    
                    # Evaluate CNN
                    cnn_pred = cnn_model.predict(X_test_img)
                    cnn_pred_classes = np.argmax(cnn_pred, axis=1)
                    cnn_accuracy = accuracy_score(y_test_img, cnn_pred_classes)
                    
                    cnn_results['GPU_CNN'] = {
                        'test_accuracy': cnn_accuracy,
                        'cv_mean': cnn_accuracy,  # Simplified
                        'cv_std': 0.0
                    }
                    
                    logger.info(f"ðŸŽ¯ GPU CNN Accuracy: {cnn_accuracy:.4f} ({cnn_accuracy*100:.2f}%)")
                    
                    if cnn_accuracy >= 0.95:
                        logger.info("ðŸŽ‰ GPU CNN ACHIEVED 95%+ ACCURACY! ðŸŽ‰")
                    
            except Exception as e:
                logger.error(f"CNN training failed: {e}")
        
        # 6. Combine all results
        all_results = {**results, **cnn_results}
        
        # 7. Create visualizations
        self.create_visualizations_gpu(all_results)
        
        # 8. Final report
        end_time = datetime.now()
        duration = end_time - start_time
        
        print("\n" + "="*80)
        print("ðŸ† GPU-OPTIMIZED RESULTS")
        print("="*80)
        
        # Sort by accuracy
        sorted_results = sorted(
            [(name, result) for name, result in all_results.items() if 'error' not in result],
            key=lambda x: x[1]['test_accuracy'],
            reverse=True
        )
        
        for name, result in sorted_results:
            acc = result['test_accuracy']
            cv = result.get('cv_mean', 0)
            print(f"ðŸŽ¯ {name}: {acc:.4f} ({acc*100:.2f}%) [CV: {cv:.4f}]")
        
        if sorted_results:
            best_name, best_result = sorted_results[0]
            best_acc = best_result['test_accuracy']
            
            print(f"\nðŸ¥‡ BEST MODEL: {best_name}")
            print(f"ðŸŽ¯ BEST ACCURACY: {best_acc:.4f} ({best_acc*100:.2f}%)")
            print(f"âš¡ TOTAL TIME: {duration}")
            
            if best_acc >= 0.95:
                print("\nðŸŽ‰ðŸŽ‰ðŸŽ‰ SUCCESS! 95%+ ACCURACY ACHIEVED! ðŸŽ‰ðŸŽ‰ðŸŽ‰")
                print("âœ… TARGET REACHED WITH GPU ACCELERATION!")
            elif best_acc >= 0.90:
                gap = 0.95 - best_acc
                print(f"\nðŸ”¥ EXCELLENT! Need only {gap:.4f} ({gap*100:.2f}%) more")
            else:
                gap = 0.95 - best_acc
                print(f"\nðŸ“ˆ Good progress! Need {gap:.4f} ({gap*100:.2f}%) more")
        
        # Save results
        with open(f"{self.output_path}/results/gpu_results.txt", 'w') as f:
            f.write("GPU-OPTIMIZED SCANNER IDENTIFICATION RESULTS\n")
            f.write("="*50 + "\n\n")
            for name, result in sorted_results:
                acc = result['test_accuracy']
                f.write(f"{name}: {acc:.4f} ({acc*100:.2f}%)\n")
            f.write(f"\nTotal time: {duration}\n")
            f.write(f"GPU used: {self.gpu_available}\n")
        
        logger.info("GPU-optimized pipeline complete!")
        
        return {
            'best_accuracy': sorted_results[0][1]['test_accuracy'] if sorted_results else 0.0,
            'best_model': sorted_results[0][0] if sorted_results else 'None',
            'gpu_used': self.gpu_available,
            'duration': duration,
            'all_results': all_results
        }

if __name__ == "__main__":
    # Test GPU setup first
    logger.info("ðŸ”¥ INITIALIZING GPU-OPTIMIZED SYSTEM...")
    
    # Run the GPU-optimized pipeline
    gpu_system = GPUOptimized95Plus()
    results = gpu_system.run_gpu_optimized_pipeline()
    
    print(f"\nðŸš€ GPU-optimized pipeline finished!")
    print(f"Best accuracy: {results['best_accuracy']:.4f}")
    print(f"GPU used: {results['gpu_used']}")
